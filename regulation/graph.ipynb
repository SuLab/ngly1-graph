{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @name graph\n",
    "# @description notebook to build the graph from different edges datasets\n",
    "# @author NÃºria Queralt Rosinach\n",
    "# @date 04-09-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "#       * DIFF WITH graph_nodes.ipynb the curated network creation:\n",
    "#              1. name of the folder to `curated_v{}`\n",
    "#              2. where the new curated and graph adapted net is saved, now in ./graph dir\n",
    "#       * review 'name' manually, specially for gene orthologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, glob, os\n",
    "import pandas as pd\n",
    "from biothings_client import get_client\n",
    "import datetime\n",
    "\n",
    "# read data\n",
    "sys.path.insert(0, './')\n",
    "\n",
    "# database version path\n",
    "version = 'v20180118'\n",
    "\n",
    "# timestamp\n",
    "today = datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate\n",
    "1. import graph edges and nodes into the graph folder\n",
    "2. check format\n",
    "3. concat\n",
    "4. save graph edges and nodes\n",
    "5. neo4j statement and concept files\n",
    "6. import graph into neo4j\n",
    "\n",
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# import graph edges into the graph folder\n",
    "# curated, monarch orthopheno connections, g2p (curated, monarch)\n",
    "# ngly1 v2 (animal model) corrected: ../curation/kylo/neo4j/networks/v20180118\n",
    "# by script: workspace/ngly1-graph/curation/kylo/neo4j/networks/concatenate_network_files.ipynb\n",
    "# Corrections: curated/papers edges/nodes files replaced \"-\" by \"_\" to concatenate them. g2p curated/monarch edges\n",
    "# lam_nodes: two phenotypes withdrawn; glcnac_nodes: two CHEM added\n",
    "# Edges/Nodes:\n",
    "#             * monarch: v20171128, statements/concepts\n",
    "#             * curated: 5 nodes, 2 papers\n",
    "#             * g2p: statements, network (curated)/monarch\n",
    "# Genes: NCBIGene\n",
    "mkdir -p graph/curated_v20180118\n",
    "cp -r ../curation/kylo/neo4j/networks/v20180118/* graph/curated_v20180118/.\n",
    "cd graph/curated_v20180118\n",
    "rm -f curated_statements.tsv ngly1_statements.tsv ngly1_concepts.tsv \n",
    "rm -f g2p_edges_network.tsv g2p_edges_monarch.tsv\n",
    "rm -f monarch_edges_v2017-11-28.tsv monarch_nodes_v2017-11-28.tsv\n",
    "cd ../..\n",
    "# checked that all have graph format like regulation and rna edges/nodes. \n",
    "# graph nodes: should query biothings to incorporate name (and for monarch: alias and summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing curated network...\n",
      "\n",
      "Drop duplicated rows...\n",
      "322\n",
      "321\n",
      "(321, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "\n",
      "ID conversion: from ngly1 curated network to monarch graph...\n",
      "\n",
      "Mapping genes to HGNC ID...\n",
      "querying 1-18...done.\n",
      "Finished.\n",
      "\n",
      "Mapping diseases to MONDO ID...\n",
      "List of curated diseases: {'DOID:10595', 'OMIM:608984', 'DOID:0060308', 'OMIM:231550', 'DOID:2476', 'DOID:5212', 'DOID:0060728', 'Orphanet:869', 'Orphanet:314381', 'OMIM:223900', 'OMIM:614653', 'DOID:11589', 'OMIM:615510', 'DOID:0050602', 'OMIM:615273'}\n",
      "(336, 9)\n",
      "\n",
      "Adding gene to protein network...\n",
      "querying 1-43...done.\n",
      "Finished.\n",
      "(42, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "(378, 9)\n",
      "\n",
      "Drop duplicated gene-protein relations...\n",
      "(362, 10)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri', 'g2p_mark'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing Monarch network...\n",
      "(226556, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing transcriptomics network...\n",
      "(386, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing tf-gene network...\n",
      "(9723, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "\n",
      "Concatenating into a graph...\n",
      "(237027, 9)\n",
      "\n",
      "Drop duplicated rows...\n",
      "(237027, 9)\n",
      "\n",
      "Saving final graph...\n",
      "(237027, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# working directory\n",
    "path = os.getcwd() + \"/graph/curated_\" + version\n",
    "\n",
    "## curated network\n",
    "print('\\nPreparing curated network...')\n",
    "# concat all curated statements in the network\n",
    "df_l = []\n",
    "for file in glob.glob('{}/*_edges.tsv'.format(path)):\n",
    "    with open(file, 'r') as f:\n",
    "        df_l.append(pd.read_table(f))\n",
    "\n",
    "curated_df = pd.concat(df_l, ignore_index=True, join=\"inner\")\n",
    "\n",
    "# ID curie normalization: curation to Monarch ID\n",
    "# subject_id\n",
    "curated_df['subject_id'] = ( curated_df.subject_id\n",
    "               .apply(lambda x:\n",
    "                      'ClinVarVariant:50962' if 'HGVS' in str(x) else     \n",
    "                       x.replace('Reactome', 'REACT') if 'Reactome' in str(x) else str(x).strip()\n",
    "                     )\n",
    ")\n",
    "# object_id\n",
    "curated_df['object_id'] = ( curated_df.object_id\n",
    "               .apply(lambda x:\n",
    "                      'ClinVarVariant:50962' if 'HGVS' in str(x) else     \n",
    "                       x.replace('Reactome', 'REACT') if 'Reactome' in str(x) else str(x).strip()\n",
    "                     )\n",
    ")\n",
    "\n",
    "# uniform columns bw monarch and curated file formats\n",
    "curated_df = curated_df[['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
    "       'reference_supporting_text', 'reference_date', 'property_label',\n",
    "       'property_description', 'property_uri']]\n",
    "\n",
    "# drop row duplicates\n",
    "print('\\nDrop duplicated rows...')\n",
    "print(len(curated_df))\n",
    "curated_df.drop_duplicates(inplace=True)\n",
    "print(len(curated_df))\n",
    "\n",
    "# save curated edges\n",
    "curated_df.fillna('NA').to_csv('{}/curated_edges_v{}.csv'.format(path,today), index=False)\n",
    "print(curated_df.shape)\n",
    "print(curated_df.columns)\n",
    "\n",
    "# ID conversion: from ngly1 curated network to monarch graph \n",
    "# script: http://localhost:8888/notebooks/workspace/ngly1-graph/regulation/curated.ipynb\n",
    "# HUMAN GENES: NCBIGene to HGNC ID using biothings\n",
    "# DISEASES: DO, OMIM, Orphanet IDs to MONDO ID manually \n",
    "# and add edges disease id to mondo id in the graph (exact match)\n",
    "# and add new mondo id nodes parsing the mondo owl ontology to extract node attributes\n",
    "print('\\nID conversion: from ngly1 curated network to monarch graph...')\n",
    "# Genes #\n",
    "print('\\nMapping genes to HGNC ID...')\n",
    "# biothings api + dictionaries\n",
    "# concepts\n",
    "concept_dct = dict()\n",
    "for i, row in curated_df.iterrows():\n",
    "    # node for subject\n",
    "    concept_dct[row['subject_id']] = 1\n",
    "    # node for object\n",
    "    concept_dct[row['object_id']] = 1\n",
    "    \n",
    "# api input\n",
    "entrez = list()\n",
    "diseases = set()\n",
    "for idx, row in concept_dct.items():\n",
    "    if ':' in idx:\n",
    "        if 'ncbigene' in idx.split(':')[0].lower():\n",
    "            entrez.append(idx.split(':')[1])\n",
    "        elif 'doid' in idx.split(':')[0].lower() or 'omim' in idx.split(':')[0].lower() or 'orphanet' in idx.split(':')[0].lower():\n",
    "            diseases.add(idx)\n",
    "entrez = list(set(entrez))\n",
    "\n",
    "# api call \n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(entrez, scopes = 'entrezgene', fields='HGNC', size=1, as_dataframe=True)\n",
    "\n",
    "# build dictionary\n",
    "ids = df.reset_index().rename(columns={'query': 'entrez'}).copy()\n",
    "entrez2hgnc_dct = dict(zip(ids.entrez, ids.HGNC))\n",
    "\n",
    "# map to hgnc\n",
    "lines = []\n",
    "for idx, row in curated_df.iterrows():\n",
    "    # subject\n",
    "    if ':' in row['subject_id']:\n",
    "        if 'NCBIGene' in row['subject_id'].split(':')[0]:\n",
    "            # human ncbi gene ids with HGNC ID\n",
    "            if str(entrez2hgnc_dct[row['subject_id'].split(':')[1]]) != 'nan':\n",
    "                row['subject_id'] = \"HGNC:\"+entrez2hgnc_dct[row['subject_id'].split(':')[1]]\n",
    "            # specific non human ncbi gene ids in the curated set\n",
    "            elif row['subject_id'] == 'NCBIGene:173028':\n",
    "                row['subject_id'] = 'WormBase:WBGene00010160'\n",
    "            elif row['subject_id'] == 'NCBIGene:11826':\n",
    "                row['subject_id'] = 'MGI:103201'\n",
    "    \n",
    "    # object\n",
    "    if ':' in row['object_id']:\n",
    "        if 'NCBIGene' in row['object_id'].split(':')[0]:\n",
    "            # human ncbi gene ids with HGNC ID\n",
    "            if str(entrez2hgnc_dct[row['object_id'].split(':')[1]]) != 'nan':\n",
    "                row['object_id'] = \"HGNC:\"+entrez2hgnc_dct[row['object_id'].split(':')[1]]\n",
    "            # specific non human ncbi gene ids in the curated set\n",
    "            elif row['object_id'] == 'NCBIGene:173028':\n",
    "                row['object_id'] = 'WormBase:WBGene00010160'\n",
    "            elif row['object_id'] == 'NCBIGene:11826':\n",
    "                row['object_id'] = 'MGI:103201'\n",
    "\n",
    "    lines.append((row))\n",
    "curated_df = pd.DataFrame.from_records(lines)\n",
    "\n",
    "# Diseases #\n",
    "print('\\nMapping diseases to MONDO ID...')\n",
    "print('List of curated diseases:',diseases)\n",
    "# add edges\n",
    "# manually: dict diseases to mondo\n",
    "d2m = {\n",
    "       'OMIM:223900': 'MONDO:0009131', \n",
    "       'DOID:2476': 'MONDO:0019064', \n",
    "       'Orphanet:869': 'MONDO:0009279', \n",
    "       'DOID:11589': 'MONDO:0009131', \n",
    "       'OMIM:614653': 'MONDO:0013839', \n",
    "       'OMIM:615510': 'MONDO:0014219', \n",
    "       'Orphanet:314381': 'MONDO:0013839', \n",
    "       'DOID:10595': 'MONDO:0015626', \n",
    "       'OMIM:608984': 'MONDO:0012166', \n",
    "       'DOID:5212': 'MONDO:0015286', \n",
    "       'OMIM:615273': 'MONDO:0014109', \n",
    "       'DOID:0060308': 'MONDO:0019502', \n",
    "       'DOID:0060728': 'MONDO:0014109', \n",
    "       'OMIM:231550': 'MONDO:0009279', \n",
    "       'DOID:0050602': 'MONDO:0009279'\n",
    "}\n",
    "\n",
    "# add equivalentTo MONDO edges\n",
    "edges_l = list()\n",
    "for disease, mondo in d2m.items():\n",
    "    edge = dict()\n",
    "    edge['subject_id'] = disease\n",
    "    edge['object_id'] = mondo\n",
    "    edge['property_id'] = 'skos:exactMatch'\n",
    "    edge['property_label'] = 'exact match'\n",
    "    edge['property_description'] = 'NA'\n",
    "    edge['property_uri'] = 'NA'\n",
    "    edge['reference_uri'] = 'https://monarchinitiative.org/disease/'+mondo\n",
    "    edge['reference_supporting_text'] = 'Manual extraction from Monarch Knowledge Graph.'\n",
    "    edge['reference_date'] = '2018-04'\n",
    "    edges_l.append(edge)\n",
    "    \n",
    "d2m_edges_df = pd.DataFrame(edges_l)\n",
    "curated_df = pd.concat([curated_df,d2m_edges_df], ignore_index=True, join=\"inner\")\n",
    "print(curated_df.shape)\n",
    "\n",
    "# add g2p network\n",
    "print('\\nAdding gene to protein network...')\n",
    "# biothings api + dictionaries\n",
    "# api input\n",
    "uniprot = list()\n",
    "for idx, row in concept_dct.items():\n",
    "    if ':' in idx:\n",
    "        if 'uniprot' in idx.split(':')[0].lower():\n",
    "            uniprot.append(idx.split(':')[1])\n",
    "uniprot = list(set(uniprot))\n",
    "\n",
    "# api call \n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(uniprot, scopes = 'uniprot', fields='HGNC', size=1, as_dataframe=True)\n",
    "\n",
    "# build dictionary\n",
    "ids = df.reset_index().rename(columns={'query': 'uniprot'}).copy()\n",
    "uniprot2hgnc_dct = dict(zip(ids.uniprot, ids.HGNC))\n",
    "\n",
    "# add equivalentTo edges\n",
    "edges_l = list()\n",
    "for uniprot, hgnc in uniprot2hgnc_dct.items():\n",
    "    if str(uniprot2hgnc_dct[uniprot]) == 'nan':\n",
    "        continue\n",
    "    edge = dict()\n",
    "    edge['subject_id'] = 'HGNC:'+hgnc\n",
    "    edge['object_id'] = 'UniProt:'+uniprot\n",
    "    edge['property_id'] = 'RO:0002205'\n",
    "    edge['property_label'] = 'has gene product'\n",
    "    edge['property_description'] = 'NA'\n",
    "    edge['property_uri'] = 'NA'\n",
    "    edge['reference_uri'] = 'http://mygene.info/clients/'\n",
    "    edge['reference_supporting_text'] = 'Automatic extraction via the python client for mygene.info services.'\n",
    "    edge['reference_date'] = today\n",
    "    edges_l.append(edge)\n",
    "    \n",
    "p2g_edges_df = pd.DataFrame(edges_l)\n",
    "print(p2g_edges_df.shape)\n",
    "print(p2g_edges_df.columns)\n",
    "curated_df = pd.concat([curated_df,p2g_edges_df], ignore_index=True, join=\"inner\")\n",
    "print(curated_df.shape)\n",
    "\n",
    "# drop g2p duplicates\n",
    "print('\\nDrop duplicated gene-protein relations...')\n",
    "# mark `has gene product` to be deleted if duplicated\n",
    "curated_df['g2p_mark'] = (\n",
    "    [ curated_df.at[idx,'property_label'] if 'has gene product' in curated_df.at[idx,'property_label'] else \n",
    "     idx for idx in curated_df.index ]\n",
    ")\n",
    "# keep first: keep the g2p manually added\n",
    "curated_df.drop_duplicates(subset=['subject_id','property_id', 'object_id', 'g2p_mark'],keep='first',inplace=True)\n",
    "\n",
    "# save curated normalized to graph edges\n",
    "path = os.getcwd() + \"/graph\"\n",
    "curated_df.fillna('NA').to_csv('{}/curated_graph_edges_v{}.csv'.format(path,today), index=False)\n",
    "print(curated_df.shape)\n",
    "print(curated_df.columns)\n",
    "\n",
    "## monarch network\n",
    "print('\\nPreparing Monarch network...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "monarch_df = pd.read_table('{}/monarch_edges_v2019-01-18.tsv'.format(path))\n",
    "print(monarch_df.shape)\n",
    "print(monarch_df.columns)\n",
    "\n",
    "## transcriptomics network\n",
    "print('\\nPreparing transcriptomics network...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "rna = pd.read_csv('{}/rna_edges_v2019-01-17.csv'.format(path))\n",
    "print(rna.shape)\n",
    "print(rna.columns)\n",
    "\n",
    "## regulation network\n",
    "print('\\nPreparing tf-gene network...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "tf_merged = pd.read_csv('{}/regulation_graph_edges_v2019-01-17.csv'.format(path))\n",
    "print(tf_merged.shape)\n",
    "print(tf_merged.columns)\n",
    "\n",
    "# concat 1) curated 2) monarch 3) RNA-seq edges\n",
    "print('\\nConcatenating into a graph...')\n",
    "statements = pd.concat([curated_df,monarch_df,rna,tf_merged], ignore_index=True, join=\"inner\")\n",
    "print(statements.shape)\n",
    "\n",
    "# drop row duplicates\n",
    "print('\\nDrop duplicated rows...')\n",
    "statements.drop_duplicates(keep='first',inplace=True)\n",
    "print(statements.shape)\n",
    "\n",
    "# add property_uri for those without but with a curie property_id annotated\n",
    "curie_dct = {\n",
    "    'ro': 'http://purl.obolibrary.org/obo/',\n",
    "    'bfo': 'http://purl.obolibrary.org/obo/',\n",
    "    'geno': 'http://purl.obolibrary.org/obo/',\n",
    "    'dc': 'http://purl.org/dc/elements/1.1/',\n",
    "    'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'skos': 'http://www.w3.org/2004/02/skos/core#',\n",
    "    'pato': 'http://purl.obolibrary.org/obo/',\n",
    "    'sio': 'http://semanticscience.org/resource/',\n",
    "    'pmid': 'https://www.ncbi.nlm.nih.gov/pubmed/',\n",
    "    'encode': 'https://www.encodeproject.org/search/?searchTerm='\n",
    "}\n",
    "for i, row in statements.iterrows():\n",
    "    if ':' in str(row['property_uri']):\n",
    "        property_uri = row['property_uri']\n",
    "    elif ':' in str(row['property_id']):\n",
    "        try:\n",
    "            property_uri = curie_dct[row['property_id'].split(':')[0].lower()]+row['property_id'].replace(':','_')\n",
    "        except KeyError:\n",
    "            property_uri = None\n",
    "            print('There is a reference curie with and unrecognized namespace:', row['property_id'])\n",
    "    else:\n",
    "        property_uri = None\n",
    "    statements.at[i, 'property_uri'] = property_uri\n",
    "    \n",
    "# save graph\n",
    "print('\\nSaving final graph...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "statements = statements[['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
    "       'reference_supporting_text', 'reference_date', 'property_label',\n",
    "       'property_description', 'property_uri']]\n",
    "print(statements.shape)\n",
    "print(statements.columns)\n",
    "statements.fillna('NA').to_csv('{}/graph_edges_v{}.csv'.format(path,today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "* Graph nodes: add name, synonyms (alias), and description (summary) from biothings, add in case the value is None or NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.insert(0,'/home/nuria/soft/utils3/ontologies')\n",
    "sys.path.insert(0,'./utils')\n",
    "import mondo_class as mondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing curated nodes...\n",
      "\n",
      "ID conversion: from ngly1 curated network to monarch graph...\n",
      "\n",
      "Mapping genes to HGNC ID...\n",
      "querying 1-18...done.\n",
      "Finished.\n",
      "\n",
      "Adding diseases described by the MONDO ontology...\n",
      "\n",
      "Adding BioThings annotation: gene names...\n",
      "symbols: 298\n",
      "querying 1-298...done.\n",
      "Finished.\n",
      "38 input query terms found dup hits:\n",
      "\t[('or', 5), ('NGLY1', 3), ('of', 16), ('1', 14), ('by', 9), ('MRS', 9), ('CSF', 8), ('acid', 4), ('B\n",
      "591 input query terms found no hit:\n",
      "\t['NGLY1-deficiency', 'misfolded', 'incompletely', 'synthesized', 'protein', 'catabolic', 'process', \n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "(298, 6)\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'description',\n",
      "       'name'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing Monarch nodes...\n",
      "\n",
      "Adding BioThings annotation: gene name, synonyms, description...\n",
      "symbols: 5889\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-5889...done.\n",
      "Finished.\n",
      "295 input query terms found dup hits:\n",
      "\t[('Shpk', 2), ('Nfe2l3', 2), ('NFE2L3', 6), ('KEAP1', 6), ('STAT5A', 4), ('HCFC1', 6), ('FOS', 6), (\n",
      "53 input query terms found no hit:\n",
      "\t['Aqp1<tm1Ask>', 'Aqp1<tm1b(EUCOMM)Wtsi>', 'si:ch211-126i22.5', 'Aqp1<tm1b(EUCOMM)Wtsi>/Aqp1<tm1b(EU\n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "(9024, 6)\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'description',\n",
      "       'name'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing transcriptomics nodes...\n",
      "(386, 6)\n",
      "Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing tf-gene nodes...\n",
      "(16963, 6)\n",
      "Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing encoding genes from ngly1 curated network...\n",
      "\n",
      "Adding BioThings annotation: gene symbol, name, synonyms, description...\n",
      "querying 1-43...done.\n",
      "Finished.\n",
      "\n",
      "Annotating nodes in the graph...\n",
      "(9365, 1)\n",
      "\n",
      "Concatenating all nodes...\n",
      "(13976, 6)\n",
      "\n",
      "Drop duplicated rows...\n",
      "(11098, 6)\n",
      "\n",
      "Drop duplicated nodes...\n",
      "(9365, 6)\n",
      "\n",
      "All graph nodes are annotated.\n",
      "Regulation nodes not in the graph: 12737\n",
      "\n",
      "Saving final graph...\n",
      "(9365, 6)\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'name',\n",
      "       'description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# working directory\n",
    "path = os.getcwd() + \"/graph/curated_\" + version\n",
    "\n",
    "## curated nodes\n",
    "print('\\nPreparing curated nodes...')\n",
    "# concat all curated concepts in the network\n",
    "df_l = []\n",
    "for file in glob.glob('{}/*_nodes.tsv'.format(path)):\n",
    "    with open(file, 'r') as f:\n",
    "        df_l.append(pd.read_table(f))\n",
    "\n",
    "curated_df = pd.concat(df_l, ignore_index=True, join=\"inner\")   \n",
    "\n",
    "# ID curie normalization: curation to Monarch ID\n",
    "curated_df['id'] = ( curated_df.id\n",
    "               .apply(lambda x:\n",
    "                      'ClinVarVariant:50962' if 'HGVS' in str(x) else     \n",
    "                       x.replace('Reactome', 'REACT') if 'Reactome' in str(x) else str(x).strip()\n",
    "                     )\n",
    ")\n",
    "\n",
    "# uniform columns bw monarch and curated file formats\n",
    "curated_df = curated_df[['id', 'semantic_groups', 'preflabel', 'synonyms', 'description']]\n",
    "\n",
    "# drop duplicates\n",
    "curated_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# save curated nodes\n",
    "curated_df.fillna('NA').to_csv('{}/curated_nodes_v{}.csv'.format(path,today), index=False)\n",
    "\n",
    "# ID conversion: from ngly1 curated network to monarch graph \n",
    "# script: http://localhost:8888/notebooks/workspace/ngly1-graph/regulation/curated.ipynb\n",
    "# HUMAN GENES: NCBIGene to HGNC ID using biothings\n",
    "# DISEASES: DO, OMIM, Orphanet IDs to MONDO ID manually \n",
    "# and add edges disease id to mondo id in the graph (exact match)\n",
    "# and add new mondo id nodes parsing the mondo owl ontology to extract node attributes\n",
    "print('\\nID conversion: from ngly1 curated network to monarch graph...')\n",
    "# Genes #\n",
    "print('\\nMapping genes to HGNC ID...')\n",
    "# biothings api + dictionaries \n",
    "# api input\n",
    "entrez = list()\n",
    "for i, row in curated_df.iterrows():\n",
    "    if ':' in row['id']:\n",
    "        if 'ncbigene' in row['id'].split(':')[0].lower():\n",
    "            entrez.append(row['id'].split(':')[1])\n",
    "entrez = list(set(entrez))\n",
    "\n",
    "# api call \n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(entrez, scopes = 'entrezgene', fields='HGNC', size=1, as_dataframe=True)\n",
    "\n",
    "# build dictionary\n",
    "ids = df.reset_index().rename(columns={'query': 'entrez'}).copy()\n",
    "entrez2hgnc_dct = dict(zip(ids.entrez, ids.HGNC))\n",
    "\n",
    "# map to hgnc\n",
    "lines = []\n",
    "for idx, row in curated_df.iterrows():\n",
    "    # subject\n",
    "    if ':' in row['id']:\n",
    "        if 'ncbigene' in row['id'].split(':')[0].lower():\n",
    "            # human ncbi gene ids with HGNC ID\n",
    "            if str(entrez2hgnc_dct[row['id'].split(':')[1]]) != 'nan':\n",
    "                row['id'] = \"HGNC:\"+entrez2hgnc_dct[row['id'].split(':')[1]]\n",
    "            # specific non human ncbi gene ids in the curated set\n",
    "            elif row['id'] == 'NCBIGene:173028':\n",
    "                row['id'] = 'WormBase:WBGene00010160'\n",
    "            elif row['id'] == 'NCBIGene:11826':\n",
    "                row['id'] = 'MGI:103201'\n",
    "\n",
    "    lines.append((row))\n",
    "curated_df = pd.DataFrame.from_records(lines)\n",
    "\n",
    "# Diseases #\n",
    "print('\\nAdding diseases described by the MONDO ontology...')\n",
    "# import mondo owl terms\n",
    "owl_f = os.getcwd() + '/../ontologies/mondo.owl'\n",
    "tm = mondo.term(owl_f)\n",
    "# extract metadata from the mondo ontology\n",
    "nodes_l = list()\n",
    "for disease, mondo in d2m.items():\n",
    "    mondo_term = tm.get_metadata_per_id(id=mondo)\n",
    "    node = dict()\n",
    "    node['id'] = mondo_term['id']\n",
    "    node['semantic_groups'] = 'DISO'\n",
    "    node['preflabel'] = mondo_term['label']\n",
    "    node['synonyms'] = mondo_term['synonyms']\n",
    "    node['description'] = mondo_term['definition']\n",
    "    nodes_l.append(node)\n",
    "    \n",
    "# add disease nodes to curated_df\n",
    "d2m_nodes_df = pd.DataFrame(nodes_l)\n",
    "d2m_nodes_df.drop_duplicates(inplace=True)\n",
    "curated_df = pd.concat([curated_df,d2m_nodes_df], ignore_index=True, join=\"inner\")\n",
    "\n",
    "# biothings: annotate name to genes\n",
    "print('\\nAdding BioThings annotation: gene names...')\n",
    "# input: (preflabel) symbol,alias\n",
    "symbols = list(curated_df.preflabel)\n",
    "print('symbols:', len(symbols))\n",
    "\n",
    "# query biothings\n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(symbols, scopes = 'symbol,alias', fields='name', size=1, as_dataframe=True)\n",
    "\n",
    "# dictionary: {symbol:name}\n",
    "ids = ( df.reset_index().rename(columns={'query':'symbol'}) )\n",
    "#ids['names'] = ids.name.apply(lambda x: x if str(x) != 'nan' else 'NA')\n",
    "#curated_s2n = dict(zip(ids.symbol, ids.names))\n",
    "#curated_df['name'] = curated_df.preflabel.apply(lambda x: curated_s2n[x] if x in curated_s2n.keys() else 'NA')\n",
    "curated_s2n = dict(zip(ids.symbol, ids.name))\n",
    "curated_df['name'] = curated_df.preflabel.apply(lambda x: curated_s2n[x] if x in curated_s2n.keys() else x)\n",
    "\n",
    "# save curated nodes\n",
    "path = os.getcwd() + \"/graph\"\n",
    "curated_df.fillna('NA').to_csv('{}/curated_graph_nodes_v{}.csv'.format(path,today), index=False)\n",
    "print(curated_df.shape)\n",
    "print(curated_df.columns)\n",
    "\n",
    "## monarch nodes\n",
    "print('\\nPreparing Monarch nodes...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "monarch_df = pd.read_table('{}/monarch_nodes_v2019-01-18.tsv'.format(path))\n",
    "\n",
    "# biothings: annotate name,synonyms,description to genes\n",
    "print('\\nAdding BioThings annotation: gene name, synonyms, description...')\n",
    "# input: (preflabel) symbol,alias\n",
    "symbols = list()\n",
    "for i, row in monarch_df.iterrows():\n",
    "    if isinstance(row['semantic_groups'],list):\n",
    "        for label in row['semantic_groups']:\n",
    "            if 'GENE' in label:\n",
    "                symbols.append(row['preflabel'])\n",
    "    else:\n",
    "        if 'GENE' in row['semantic_groups']:\n",
    "            symbols.append(row['preflabel'])\n",
    "print('symbols:', len(symbols))\n",
    "\n",
    "# query biothings\n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(symbols, scopes = 'symbol,alias', fields='name,alias,summary', size=1, as_dataframe=True)\n",
    "\n",
    "# dictionary: {symbol:name}\n",
    "ids = ( df.reset_index().rename(columns={'query':'symbol'}) )\n",
    "#ids['names'] = ids.name.apply(lambda x: x if str(x) != 'nan' else 'NA')\n",
    "ids['synonyms'] = ids.alias.apply(lambda x: x if str(x) != 'nan' else 'NA')\n",
    "ids['description'] = ids.summary.apply(lambda x: x if str(x) != 'nan' else 'NA')\n",
    "#monarch_s2n = dict(zip(ids.symbol, ids.names))\n",
    "monarch_s2n = dict(zip(ids.symbol, ids.name))\n",
    "monarch_s2s = dict(zip(ids.symbol, ids.synonyms))\n",
    "monarch_s2d = dict(zip(ids.symbol, ids.description))\n",
    "#monarch_df['name'] = monarch_df.preflabel.apply(lambda x: monarch_s2n[x] if x in monarch_s2n.keys() else 'NA')\n",
    "monarch_df['name'] = monarch_df.preflabel.apply(lambda x: monarch_s2n[x] if x in monarch_s2n.keys() else x)\n",
    "monarch_df['synonyms'] = monarch_df.preflabel.apply(lambda x: monarch_s2s[x] if x in monarch_s2s.keys() else 'NA')\n",
    "monarch_df['description'] = monarch_df.preflabel.apply(lambda x: monarch_s2d[x] if x in monarch_s2d.keys() else 'NA')\n",
    "print(monarch_df.shape)\n",
    "print(monarch_df.columns)\n",
    "\n",
    "## rna nodes\n",
    "print('\\nPreparing transcriptomics nodes...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "rna_df = pd.read_csv('{}/rna_nodes_v2019-01-17.csv'.format(path))\n",
    "print(rna_df.shape)\n",
    "print(rna_df.columns)\n",
    "\n",
    "## tf nodes: i only want tf merged nodes annotated\n",
    "print('\\nPreparing tf-gene nodes...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "tf_df = pd.read_csv('{}/regulation_nodes_v2019-01-17.csv'.format(path))\n",
    "print(tf_df.shape)\n",
    "print(tf_df.columns)\n",
    "\n",
    "## Annotating curated gene nodes from P-G edges\n",
    "print('\\nPreparing encoding genes from ngly1 curated network...')\n",
    "# biothings api + dictionaries \n",
    "print('\\nAdding BioThings annotation: gene symbol, name, synonyms, description...')\n",
    "# api input\n",
    "uniprot = set()\n",
    "for i, row in curated_df.iterrows():\n",
    "    if ':' in row['id']:\n",
    "        if 'uniprot' in row['id'].split(':')[0].lower():\n",
    "            uniprot.add(row['id'].split(':')[1])\n",
    "\n",
    "# api call \n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(uniprot, scopes = 'uniprot', fields='HGNC,symbol,name,alias,summary', size=1, as_dataframe=True)\n",
    "\n",
    "# build a list of nodes as list of dict, i.e a df, where a dict is a node\n",
    "nodes_l = list()\n",
    "for i,concept in df.iterrows():\n",
    "    if str(concept['HGNC']) != 'nan':\n",
    "        node = dict()\n",
    "        node['id'] = 'HGNC:'+concept['HGNC']\n",
    "        node['semantic_groups'] = 'GENE'\n",
    "        node['preflabel'] = concept['symbol'] \n",
    "        node['name'] = concept['name'] \n",
    "        node['synonyms'] = '|'.join(list(concept['alias'])) if isinstance(concept['alias'], list) else concept['alias']\n",
    "        node['description'] = concept['summary']\n",
    "        nodes_l.append(node)\n",
    "    \n",
    "# structure as dataframe\n",
    "p2g_nodes_df = pd.DataFrame(nodes_l)\n",
    "p2g_nodes_df = p2g_nodes_df.fillna('NA')\n",
    "p2g_nodes_df.drop_duplicates(inplace=True)\n",
    "\n",
    "## Annotating nodes in the graph\n",
    "print('\\nAnnotating nodes in the graph...')\n",
    "# extracting nodes in the graph\n",
    "st_nodes_l = pd.concat([statements.subject_id,statements.object_id], ignore_index=True)\n",
    "st_nodes_l.drop_duplicates(inplace=True)\n",
    "st_nodes_df = pd.DataFrame({'id': st_nodes_l})\n",
    "print(st_nodes_df.shape)\n",
    "\n",
    "# annotating nodes \n",
    "curated_nodes = pd.merge(curated_df,st_nodes_df,how='inner',on='id')\n",
    "monarch_nodes = pd.merge(monarch_df,st_nodes_df,how='inner',on='id')\n",
    "rna_nodes = pd.merge(rna_df,st_nodes_df,how='inner',on='id')\n",
    "regulation_nodes = pd.merge(tf_df,st_nodes_df,how='inner', on='id')\n",
    "p2g_nodes = pd.merge(p2g_nodes_df,st_nodes_df,how='inner', on='id')\n",
    "\n",
    "# concat all, (importantly, concatenate first curated concepts with extended definitions)\n",
    "print('\\nConcatenating all nodes...')\n",
    "nodes = pd.concat([curated_nodes,monarch_nodes,rna_nodes,regulation_nodes,p2g_nodes], ignore_index=True, join=\"inner\")\n",
    "print(nodes.shape)\n",
    "\n",
    "# drop duplicated rows\n",
    "print('\\nDrop duplicated rows...')\n",
    "nodes['synonyms'] = nodes.synonyms.apply(lambda x: str('|'.join(x)) if isinstance(x,list) else x)\n",
    "nodes.drop_duplicates(keep='first',inplace=True)\n",
    "print(nodes.shape)\n",
    "\n",
    "# drop duplicated nodes (keep first row (the curated), remove others (monarch))\n",
    "print('\\nDrop duplicated nodes...')\n",
    "nodes.drop_duplicates(subset=['id'],keep='first',inplace=True)\n",
    "print(nodes.shape)\n",
    "\n",
    "# check\n",
    "if len(set(st_nodes_df.id)) != len(set(nodes.id)):\n",
    "    print('\\nThere is a problem in the annotation of nodes.\\nThe number of annotated nodes is different than the number of nodes in the graph.')\n",
    "    print('Curated nodes not in the graph: {}'.format(set(curated_df.id)-set(curated_nodes.id)))\n",
    "    print('Monarch nodes not in the graph: {}'.format(set(monarch_df.id)-set(monarch_nodes.id)))\n",
    "    print('RNA-seq nodes not in the graph: {}'.format(set(rna_df.id)-set(rna_nodes.id)))\n",
    "    print('Regulation nodes not in the graph: {}'.format(len(set(tf_df.id)-set(regulation_nodes.id))))\n",
    "else:\n",
    "    print('\\nAll graph nodes are annotated.')\n",
    "    print('Regulation nodes not in the graph: {}'.format(len(set(tf_df.id)-set(regulation_nodes.id))))\n",
    "          \n",
    "## biothings\n",
    "# add attributes\n",
    "\n",
    "# all genes/proteins => add entrez|uniprot\n",
    "\n",
    "# save graph nodes\n",
    "print('\\nSaving final graph...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "nodes = nodes[['id','semantic_groups','preflabel','synonyms','name','description']]\n",
    "nodes['synonyms'] = nodes.synonyms.apply(lambda x: str('|'.join(x)) if isinstance(x,list) else x)\n",
    "print(nodes.shape)\n",
    "print(nodes.columns)\n",
    "nodes.fillna('NA').to_csv('{}/graph_nodes_v{}.csv'.format(path,today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEO4J import\n",
    "### save neo4j files\n",
    "save network to upload to neo4j: save with sep=',' (csv), and fill empty fields with default values 'NA'\n",
    "#### edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving neo4j graph files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/anaconda3/envs/avalanche/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "(237027, 9)\n"
     ]
    }
   ],
   "source": [
    "# save graph\n",
    "print('Saving neo4j graph files...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "statements = pd.read_csv('{}/graph_edges_v2019-01-18.csv'.format(path))\n",
    "\n",
    "\n",
    "# format edges attributes \n",
    "print(statements.columns)\n",
    "statements = statements[['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
    "       'reference_supporting_text', 'reference_date', 'property_label',\n",
    "       'property_description', 'property_uri']]\n",
    "print(statements.columns)\n",
    "print(statements.shape)\n",
    "\n",
    "# format header\n",
    "statements = ( \n",
    "        statements\n",
    "            .rename(columns = {\n",
    "                'subject_id': ':START_ID',\n",
    "                'property_id': ':TYPE',\n",
    "                'object_id': ':END_ID',\n",
    "                'property_description': 'property_description:IGNORE'\n",
    "            }\n",
    "            )\n",
    ")\n",
    "\n",
    "# working directory\n",
    "path = os.getcwd() + \"/neo4j\" \n",
    "if not os.path.isdir(path): os.makedirs(path)\n",
    "\n",
    "# save neo4j statements\n",
    "statements.fillna('NA').to_csv('{}/ngly1_statements_v{}.csv'.format(path,today), index=False)\n",
    "statements.fillna('NA').to_csv('{}/ngly1_statements.csv'.format(path), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving neo4j graph files...\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'name',\n",
      "       'description'],\n",
      "      dtype='object')\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'name',\n",
      "       'description'],\n",
      "      dtype='object')\n",
      "(9365, 6)\n"
     ]
    }
   ],
   "source": [
    "# save graph\n",
    "print('Saving neo4j graph files...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "concepts = pd.read_csv('{}/graph_nodes_v2019-01-18.csv'.format(path))\n",
    "\n",
    "\n",
    "# format nodes attributes \n",
    "print(concepts.columns)\n",
    "concepts = concepts[['id','semantic_groups','preflabel','synonyms','name','description']]\n",
    "print(concepts.columns)\n",
    "print(concepts.shape)\n",
    "\n",
    "# format header\n",
    "concepts = ( \n",
    "        concepts\n",
    "            .rename(columns = {\n",
    "                'id': 'id:ID',\n",
    "                'semantic_groups': ':LABEL',\n",
    "                'synonyms': 'synonyms:IGNORE'\n",
    "            }\n",
    "            )\n",
    ")\n",
    "\n",
    "# working directory\n",
    "path = os.getcwd() + \"/neo4j\" \n",
    "if not os.path.isdir(path): os.makedirs(path)\n",
    "\n",
    "# save neo4j statements\n",
    "concepts.fillna('NA').to_csv('{}/ngly1_concepts_v{}.csv'.format(path,today), index=False)\n",
    "concepts.fillna('NA').to_csv('{}/ngly1_concepts.csv'.format(path), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import graph into neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The graph is imported into the server. The server is running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def do_import(neo4j_path):\n",
    "    \"\"\"This function executes the import of the graph into the neo4j server instance.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # cp graph to import\n",
    "        path_to_import = neo4j_path + '/import/ngly1'\n",
    "        cmd = 'cp neo4j/* {}'.format(path_to_import)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        # stop neo4j\n",
    "        cmd = '{}/bin/neo4j stop'.format(neo4j_path)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        # rm any database in the database dir\n",
    "        cmd = 'rm -rf {}/data/databases/graph.db/*'.format(neo4j_path)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        # cd import dir files path\n",
    "        cmd = 'cd {}'.format(path_to_import)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        #  neo4j-import\n",
    "        cmd = '{}/bin/neo4j-import --into {}/data/databases/graph.db --id-type string --nodes {}/ngly1_concepts.csv --relationships {}/ngly1_statements.csv'.format(neo4j_path, neo4j_path, path_to_import, path_to_import)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        # start neo4j from database dir\n",
    "        cmd = 'cd {}/data/databases/graph.db'.format(neo4j_path)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        cmd = '{}/bin/neo4j start'.format(neo4j_path)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "    except:\n",
    "        print('error: {}'.format(sys.exc_info()[0]))\n",
    "        raise\n",
    "    else:\n",
    "        return print('\\nThe graph is imported into the server. The server is running.\\n')\n",
    "    \n",
    "# import graph into neo4j\n",
    "neo4j_path = '/home/nuria/workspace/ngly1-graph/neo4j-graphs/neo4j/neo4j-regulation-v3.2'\n",
    "do_import(neo4j_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
