{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @name graph_nodes\n",
    "# @description notebook to build the graph nodes, needed input to calculate the monarch connections for the graph (pre-graph)\n",
    "# @author NÃºria Queralt Rosinach\n",
    "# @date 01-17-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "#       * review 'name' manually, specially for gene orthologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, glob, os\n",
    "import pandas as pd\n",
    "from biothings_client import get_client\n",
    "import datetime\n",
    "\n",
    "# read data\n",
    "sys.path.insert(0, './')\n",
    "\n",
    "# database version path\n",
    "version = 'v20180118'\n",
    "\n",
    "# timestamp\n",
    "today = datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate\n",
    "1. import graph edges and nodes into the graph folder\n",
    "2. check format\n",
    "3. concat(edges and nodes): first curated > monarch > regulation\n",
    "* drop duplicated rows and concepts in edges/nodes files\n",
    "4. save graph edges and nodes\n",
    "5. format for neo4j statements and concepts files\n",
    "* save neo4j files\n",
    "6. import graph into neo4j\n",
    "\n",
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# import graph edges into the graph folder\n",
    "# curated, monarch orthopheno connections, g2p (curated, monarch)\n",
    "# ngly1 v2 (animal model) corrected: ../curation/kylo/neo4j/networks/v20180118\n",
    "# by script: workspace/ngly1-graph/curation/kylo/neo4j/networks/concatenate_network_files.ipynb\n",
    "# Corrections: curated/papers edges/nodes files replaced \"-\" by \"_\" to concatenate them. g2p curated/monarch edges\n",
    "# lam_nodes: two phenotypes withdrawn; glcnac_nodes: two CHEM added\n",
    "# Edges/Nodes:\n",
    "#             * monarch: v20171128, statements/concepts\n",
    "#             * curated: 5 nodes, 2 papers\n",
    "#             * g2p: statements, network (curated)/monarch\n",
    "# Genes: NCBIGene\n",
    "mkdir -p graph\n",
    "cp -r ../curation/kylo/neo4j/networks/v20180118 graph/.\n",
    "cd graph/v20180118\n",
    "rm -f curated_statements.tsv ngly1_statements.tsv ngly1_concepts.tsv \n",
    "rm -f g2p_edges_network.tsv g2p_edges_monarch.tsv\n",
    "rm -f monarch_edges_v2017-11-28.tsv monarch_nodes_v2017-11-28.tsv\n",
    "cd ../..\n",
    "# checked that all have graph format like regulation and rna edges/nodes. \n",
    "# graph nodes: should query biothings to incorporate name (and for monarch: alias and summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing curated network...\n",
      "\n",
      "Drop duplicated rows...\n",
      "322\n",
      "321\n",
      "(321, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "\n",
      "ID conversion: from ngly1 curated network to monarch graph...\n",
      "\n",
      "Mapping genes to HGNC ID...\n",
      "querying 1-18...done.\n",
      "Finished.\n",
      "\n",
      "Mapping diseases to MONDO ID...\n",
      "List of curated diseases: {'Orphanet:314381', 'OMIM:223900', 'DOID:11589', 'OMIM:614653', 'DOID:0060308', 'Orphanet:869', 'DOID:0060728', 'DOID:5212', 'OMIM:608984', 'OMIM:615510', 'DOID:2476', 'OMIM:231550', 'DOID:0050602', 'DOID:10595', 'OMIM:615273'}\n",
      "(336, 9)\n",
      "\n",
      "Adding gene to protein network...\n",
      "querying 1-43...done.\n",
      "Finished.\n",
      "(42, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "(378, 9)\n",
      "\n",
      "Drop duplicated gene-protein relations...\n",
      "(362, 10)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri', 'g2p_mark'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing Monarch network...\n",
      "(32715, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing transcriptomics network...\n",
      "(386, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing tf-gene network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/anaconda3/envs/avalanche/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197267, 9)\n",
      "Index(['object_id', 'property_description', 'property_id', 'property_label',\n",
      "       'property_uri', 'reference_date', 'reference_supporting_text',\n",
      "       'reference_uri', 'subject_id'],\n",
      "      dtype='object')\n",
      "\n",
      "Concatenating into a graph...\n",
      "(33463, 9)\n",
      "\n",
      "Drop duplicated rows...\n",
      "(33463, 9)\n",
      "\n",
      "Merging tf-gene network to the graph...\n",
      "(9723, 9)\n",
      "\n",
      "Saving tf merged edges...\n",
      "(43186, 9)\n",
      "\n",
      "Drop duplicated rows...\n",
      "(43186, 9)\n",
      "\n",
      "Saving final graph...\n",
      "(43186, 9)\n",
      "Index(['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
      "       'reference_supporting_text', 'reference_date', 'property_label',\n",
      "       'property_description', 'property_uri'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# working directory\n",
    "path = os.getcwd() + \"/graph/\" + version\n",
    "\n",
    "## curated network\n",
    "print('\\nPreparing curated network...')\n",
    "# concat all curated statements in the network\n",
    "df_l = []\n",
    "for file in glob.glob('{}/*_edges.tsv'.format(path)):\n",
    "    with open(file, 'r') as f:\n",
    "        df_l.append(pd.read_table(f))\n",
    "\n",
    "curated_df = pd.concat(df_l, ignore_index=True, join=\"inner\")\n",
    "\n",
    "# ID curie normalization: curation to Monarch ID\n",
    "# subject_id\n",
    "curated_df['subject_id'] = ( curated_df.subject_id\n",
    "               .apply(lambda x:\n",
    "                      'ClinVarVariant:50962' if 'HGVS' in str(x) else     \n",
    "                       x.replace('Reactome', 'REACT') if 'Reactome' in str(x) else str(x).strip()\n",
    "                     )\n",
    ")\n",
    "# object_id\n",
    "curated_df['object_id'] = ( curated_df.object_id\n",
    "               .apply(lambda x:\n",
    "                      'ClinVarVariant:50962' if 'HGVS' in str(x) else     \n",
    "                       x.replace('Reactome', 'REACT') if 'Reactome' in str(x) else str(x).strip()\n",
    "                     )\n",
    ")\n",
    "\n",
    "# uniform columns bw monarch and curated file formats\n",
    "curated_df = curated_df[['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
    "       'reference_supporting_text', 'reference_date', 'property_label',\n",
    "       'property_description', 'property_uri']]\n",
    "\n",
    "# drop row duplicates\n",
    "print('\\nDrop duplicated rows...')\n",
    "print(len(curated_df))\n",
    "curated_df.drop_duplicates(inplace=True)\n",
    "print(len(curated_df))\n",
    "\n",
    "# save curated edges\n",
    "curated_df.fillna('NA').to_csv('{}/curated_edges_v{}.csv'.format(path,today), index=False)\n",
    "print(curated_df.shape)\n",
    "print(curated_df.columns)\n",
    "\n",
    "# ID conversion: from ngly1 curated network to monarch graph \n",
    "# script: http://localhost:8888/notebooks/workspace/ngly1-graph/regulation/curated.ipynb\n",
    "# HUMAN GENES: NCBIGene to HGNC ID using biothings\n",
    "# DISEASES: DO, OMIM, Orphanet IDs to MONDO ID manually \n",
    "# and add edges disease id to mondo id in the graph (exact match)\n",
    "# and add new mondo id nodes parsing the mondo owl ontology to extract node attributes\n",
    "print('\\nID conversion: from ngly1 curated network to monarch graph...')\n",
    "# Genes #\n",
    "print('\\nMapping genes to HGNC ID...')\n",
    "# biothings api + dictionaries\n",
    "# concepts\n",
    "concept_dct = dict()\n",
    "for i, row in curated_df.iterrows():\n",
    "    # node for subject\n",
    "    concept_dct[row['subject_id']] = 1\n",
    "    # node for object\n",
    "    concept_dct[row['object_id']] = 1\n",
    "    \n",
    "# api input\n",
    "entrez = list()\n",
    "diseases = set()\n",
    "for idx, row in concept_dct.items():\n",
    "    if ':' in idx:\n",
    "        if 'ncbigene' in idx.split(':')[0].lower():\n",
    "            entrez.append(idx.split(':')[1])\n",
    "        elif 'doid' in idx.split(':')[0].lower() or 'omim' in idx.split(':')[0].lower() or 'orphanet' in idx.split(':')[0].lower():\n",
    "            diseases.add(idx)\n",
    "entrez = list(set(entrez))\n",
    "\n",
    "# api call \n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(entrez, scopes = 'entrezgene', fields='HGNC', size=1, as_dataframe=True)\n",
    "\n",
    "# build dictionary\n",
    "ids = df.reset_index().rename(columns={'query': 'entrez'}).copy()\n",
    "entrez2hgnc_dct = dict(zip(ids.entrez, ids.HGNC))\n",
    "\n",
    "# map to hgnc\n",
    "lines = []\n",
    "for idx, row in curated_df.iterrows():\n",
    "    # subject\n",
    "    if ':' in row['subject_id']:\n",
    "        if 'NCBIGene' in row['subject_id'].split(':')[0]:\n",
    "            # human ncbi gene ids with HGNC ID\n",
    "            if str(entrez2hgnc_dct[row['subject_id'].split(':')[1]]) != 'nan':\n",
    "                row['subject_id'] = \"HGNC:\"+entrez2hgnc_dct[row['subject_id'].split(':')[1]]\n",
    "            # specific non human ncbi gene ids in the curated set\n",
    "            elif row['subject_id'] == 'NCBIGene:173028':\n",
    "                row['subject_id'] = 'WormBase:WBGene00010160'\n",
    "            elif row['subject_id'] == 'NCBIGene:11826':\n",
    "                row['subject_id'] = 'MGI:103201'\n",
    "    \n",
    "    # object\n",
    "    if ':' in row['object_id']:\n",
    "        if 'NCBIGene' in row['object_id'].split(':')[0]:\n",
    "            # human ncbi gene ids with HGNC ID\n",
    "            if str(entrez2hgnc_dct[row['object_id'].split(':')[1]]) != 'nan':\n",
    "                row['object_id'] = \"HGNC:\"+entrez2hgnc_dct[row['object_id'].split(':')[1]]\n",
    "            # specific non human ncbi gene ids in the curated set\n",
    "            elif row['object_id'] == 'NCBIGene:173028':\n",
    "                row['object_id'] = 'WormBase:WBGene00010160'\n",
    "            elif row['object_id'] == 'NCBIGene:11826':\n",
    "                row['object_id'] = 'MGI:103201'\n",
    "\n",
    "    lines.append((row))\n",
    "curated_df = pd.DataFrame.from_records(lines)\n",
    "\n",
    "# Diseases #\n",
    "print('\\nMapping diseases to MONDO ID...')\n",
    "print('List of curated diseases:',diseases)\n",
    "# add edges\n",
    "# manually: dict diseases to mondo\n",
    "d2m = {\n",
    "       'OMIM:223900': 'MONDO:0009131', \n",
    "       'DOID:2476': 'MONDO:0019064', \n",
    "       'Orphanet:869': 'MONDO:0009279', \n",
    "       'DOID:11589': 'MONDO:0009131', \n",
    "       'OMIM:614653': 'MONDO:0013839', \n",
    "       'OMIM:615510': 'MONDO:0014219', \n",
    "       'Orphanet:314381': 'MONDO:0013839', \n",
    "       'DOID:10595': 'MONDO:0015626', \n",
    "       'OMIM:608984': 'MONDO:0012166', \n",
    "       'DOID:5212': 'MONDO:0015286', \n",
    "       'OMIM:615273': 'MONDO:0014109', \n",
    "       'DOID:0060308': 'MONDO:0019502', \n",
    "       'DOID:0060728': 'MONDO:0014109', \n",
    "       'OMIM:231550': 'MONDO:0009279', \n",
    "       'DOID:0050602': 'MONDO:0009279'\n",
    "}\n",
    "\n",
    "# add equivalentTo MONDO edges\n",
    "edges_l = list()\n",
    "for disease, mondo in d2m.items():\n",
    "    edge = dict()\n",
    "    edge['subject_id'] = disease\n",
    "    edge['object_id'] = mondo\n",
    "    edge['property_id'] = 'skos:exactMatch'\n",
    "    edge['property_label'] = 'exact match'\n",
    "    edge['property_description'] = 'NA'\n",
    "    edge['property_uri'] = 'NA'\n",
    "    edge['reference_uri'] = 'https://monarchinitiative.org/disease/'+mondo\n",
    "    edge['reference_supporting_text'] = 'Manual extraction from Monarch Knowledge Graph.'\n",
    "    edge['reference_date'] = '2018-04'\n",
    "    edges_l.append(edge)\n",
    "    \n",
    "d2m_edges_df = pd.DataFrame(edges_l)\n",
    "curated_df = pd.concat([curated_df,d2m_edges_df], ignore_index=True, join=\"inner\")\n",
    "print(curated_df.shape)\n",
    "\n",
    "# add g2p network\n",
    "print('\\nAdding gene to protein network...')\n",
    "# biothings api + dictionaries\n",
    "# api input\n",
    "uniprot = list()\n",
    "for idx, row in concept_dct.items():\n",
    "    if ':' in idx:\n",
    "        if 'uniprot' in idx.split(':')[0].lower():\n",
    "            uniprot.append(idx.split(':')[1])\n",
    "uniprot = list(set(uniprot))\n",
    "\n",
    "# api call \n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(uniprot, scopes = 'uniprot', fields='HGNC', size=1, as_dataframe=True)\n",
    "\n",
    "# build dictionary\n",
    "ids = df.reset_index().rename(columns={'query': 'uniprot'}).copy()\n",
    "uniprot2hgnc_dct = dict(zip(ids.uniprot, ids.HGNC))\n",
    "\n",
    "# add equivalentTo edges\n",
    "edges_l = list()\n",
    "for uniprot, hgnc in uniprot2hgnc_dct.items():\n",
    "    if str(uniprot2hgnc_dct[uniprot]) == 'nan':\n",
    "        continue\n",
    "    edge = dict()\n",
    "    edge['subject_id'] = 'HGNC:'+hgnc\n",
    "    edge['object_id'] = 'UniProt:'+uniprot\n",
    "    edge['property_id'] = 'RO:0002205'\n",
    "    edge['property_label'] = 'has gene product'\n",
    "    edge['property_description'] = 'NA'\n",
    "    edge['property_uri'] = 'NA'\n",
    "    edge['reference_uri'] = 'http://mygene.info/clients/'\n",
    "    edge['reference_supporting_text'] = 'Automatic extraction via the python client for mygene.info services.'\n",
    "    edge['reference_date'] = today\n",
    "    edges_l.append(edge)\n",
    "    \n",
    "p2g_edges_df = pd.DataFrame(edges_l)\n",
    "print(p2g_edges_df.shape)\n",
    "print(p2g_edges_df.columns)\n",
    "curated_df = pd.concat([curated_df,p2g_edges_df], ignore_index=True, join=\"inner\")\n",
    "print(curated_df.shape)\n",
    "\n",
    "# drop g2p duplicates\n",
    "print('\\nDrop duplicated gene-protein relations...')\n",
    "# mark `has gene product` to be deleted if duplicated\n",
    "curated_df['g2p_mark'] = (\n",
    "    [ curated_df.at[idx,'property_label'] if 'has gene product' in curated_df.at[idx,'property_label'] else \n",
    "     idx for idx in curated_df.index ]\n",
    ")\n",
    "# keep first: keep the g2p manually added\n",
    "curated_df.drop_duplicates(subset=['subject_id','property_id', 'object_id', 'g2p_mark'],keep='first',inplace=True)\n",
    "\n",
    "# save curated normalized to graph edges\n",
    "curated_df.fillna('NA').to_csv('{}/curated_graph_edges_v{}.csv'.format(path,today), index=False)\n",
    "print(curated_df.shape)\n",
    "print(curated_df.columns)\n",
    "\n",
    "## monarch network\n",
    "print('\\nPreparing Monarch network...')\n",
    "#path = os.getcwd() + \"/../connectivity/1shell-animalModel-hgnc/add-connections-to-net\"\n",
    "path = os.getcwd() + \"/../monarch/1shell-animal/add-connections-to-net\"\n",
    "monarch_df = pd.read_table('{}/monarch_edges_v2019-01-16.tsv'.format(path))\n",
    "print(monarch_df.shape)\n",
    "print(monarch_df.columns)\n",
    "\n",
    "## transcriptomics network\n",
    "print('\\nPreparing transcriptomics network...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "rna = pd.read_csv('{}/rna_edges_v2019-01-17.csv'.format(path))\n",
    "print(rna.shape)\n",
    "print(rna.columns)\n",
    "\n",
    "## regulation network\n",
    "print('\\nPreparing tf-gene network...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "tf = pd.read_csv('{}/regulation_edges_v2019-01-17.csv'.format(path))\n",
    "print(tf.shape)\n",
    "print(tf.columns)\n",
    "\n",
    "# concat 1) curated 2) monarch 3) RNA-seq edges\n",
    "print('\\nConcatenating into a graph...')\n",
    "statements = pd.concat([curated_df,monarch_df,rna], ignore_index=True, join=\"inner\")\n",
    "print(statements.shape)\n",
    "\n",
    "# drop row duplicates\n",
    "print('\\nDrop duplicated rows...')\n",
    "statements.drop_duplicates(keep='first',inplace=True)\n",
    "print(statements.shape)\n",
    "\n",
    "## merge graph & tf\n",
    "# merge: 4 merges\n",
    "print('\\nMerging tf-gene network to the graph...')\n",
    "# merge1: L_sub  &  tf_sub\n",
    "merge1 = pd.merge(statements,tf, how='inner', left_on='subject_id', right_on='subject_id', suffixes=('_graph', '_tf'))\n",
    "\n",
    "# merge2: L_obj  &  tf_sub\n",
    "merge2 = pd.merge(statements,tf, how='inner', left_on='object_id', right_on='subject_id', suffixes=('_graph', '_tf'))\n",
    "\n",
    "# merge3: L_sub  &  tf_obj\n",
    "merge3 = pd.merge(statements,tf, how='inner', left_on='subject_id', right_on='object_id', suffixes=('_graph', '_tf'))\n",
    "\n",
    "# merge4: L_obj  &  tf_obj\n",
    "merge4 = pd.merge(statements,tf, how='inner', left_on='object_id', right_on='object_id', suffixes=('_graph', '_tf'))\n",
    "\n",
    "# prepare merged edges: slice tf edges from merge\n",
    "# merge1\n",
    "merge1_clean = (merge1\n",
    "                [['subject_id', 'property_id_tf', 'object_id_tf', 'reference_uri_tf',\n",
    "       'reference_supporting_text_tf', 'reference_date_tf', 'property_label_tf',\n",
    "       'property_description_tf', 'property_uri_tf']]\n",
    "                .rename(columns={\n",
    "                    'property_id_tf': 'property_id', \n",
    "                    'object_id_tf': 'object_id', \n",
    "                    'reference_uri_tf': 'reference_uri',\n",
    "       'reference_supporting_text_tf': 'reference_supporting_text', \n",
    "                    'reference_date_tf': 'reference_date', \n",
    "                    'property_label_tf': 'property_label',\n",
    "       'property_description_tf': 'property_description', \n",
    "                    'property_uri_tf': 'property_uri'\n",
    "                })\n",
    "               )\n",
    "\n",
    "# merge2\n",
    "merge2_clean = (merge2\n",
    "                [['subject_id_tf', 'property_id_tf', 'object_id_tf', 'reference_uri_tf',\n",
    "       'reference_supporting_text_tf', 'reference_date_tf', 'property_label_tf',\n",
    "       'property_description_tf', 'property_uri_tf']]\n",
    "                .rename(columns={\n",
    "                    'subject_id_tf': 'subject_id', \n",
    "                    'property_id_tf': 'property_id', \n",
    "                    'object_id_tf': 'object_id',\n",
    "                    'reference_uri_tf': 'reference_uri',\n",
    "       'reference_supporting_text_tf': 'reference_supporting_text', \n",
    "                    'reference_date_tf': 'reference_date', \n",
    "                    'property_label_tf': 'property_label',\n",
    "       'property_description_tf': 'property_description', \n",
    "                    'property_uri_tf': 'property_uri'\n",
    "                })\n",
    "               )\n",
    "\n",
    "# merge3\n",
    "merge3_clean = (merge3\n",
    "                [['subject_id_tf', 'property_id_tf', 'object_id_tf', 'reference_uri_tf',\n",
    "       'reference_supporting_text_tf', 'reference_date_tf', 'property_label_tf',\n",
    "       'property_description_tf', 'property_uri_tf']]\n",
    "                .rename(columns={\n",
    "                    'subject_id_tf': 'subject_id',\n",
    "                    'property_id_tf': 'property_id', \n",
    "                    'object_id_tf': 'object_id', \n",
    "                    'reference_uri_tf': 'reference_uri',\n",
    "       'reference_supporting_text_tf': 'reference_supporting_text', \n",
    "                    'reference_date_tf': 'reference_date', \n",
    "                    'property_label_tf': 'property_label',\n",
    "       'property_description_tf': 'property_description', \n",
    "                    'property_uri_tf': 'property_uri'\n",
    "                })\n",
    "               )\n",
    "\n",
    "# merge4\n",
    "merge4_clean = (merge4\n",
    "                [['subject_id_tf', 'property_id_tf', 'object_id', 'reference_uri_tf',\n",
    "       'reference_supporting_text_tf', 'reference_date_tf', 'property_label_tf',\n",
    "       'property_description_tf', 'property_uri_tf']]\n",
    "                .rename(columns={\n",
    "                    'subject_id_tf': 'subject_id',\n",
    "                    'property_id_tf': 'property_id',\n",
    "                    'reference_uri_tf': 'reference_uri',\n",
    "       'reference_supporting_text_tf': 'reference_supporting_text', \n",
    "                    'reference_date_tf': 'reference_date', \n",
    "                    'property_label_tf': 'property_label',\n",
    "       'property_description_tf': 'property_description', \n",
    "                    'property_uri_tf': 'property_uri'\n",
    "                })\n",
    "               )\n",
    "\n",
    "## concat merged edges to statements (<= curated+monarch+rna)\n",
    "# concat all 4 merges to merged edges\n",
    "merged = pd.concat([merge1_clean,merge2_clean,merge3_clean,merge4_clean], ignore_index=True, join=\"inner\")\n",
    "\n",
    "# drop duplicates\n",
    "merged.drop_duplicates(inplace=True)\n",
    "print(merged.shape)\n",
    "\n",
    "# save graph\n",
    "print('\\nSaving tf merged edges...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "merged.fillna('NA').to_csv('{}/regulation_graph_edges_v{}.csv'.format(path,today), index=False)\n",
    "\n",
    "# concat merged to statements\n",
    "statements = pd.concat([statements,merged], ignore_index=True, join=\"inner\")\n",
    "print(statements.shape)\n",
    "\n",
    "# drop duplicates\n",
    "print('\\nDrop duplicated rows...')\n",
    "statements.drop_duplicates(keep='first',inplace=True)\n",
    "print(statements.shape)\n",
    "\n",
    "# add property_uri for those without but with a curie property_id annotated\n",
    "curie_dct = {\n",
    "    'ro': 'http://purl.obolibrary.org/obo/',\n",
    "    'bfo': 'http://purl.obolibrary.org/obo/',\n",
    "    'geno': 'http://purl.obolibrary.org/obo/',\n",
    "    'dc': 'http://purl.org/dc/elements/1.1/',\n",
    "    'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'skos': 'http://www.w3.org/2004/02/skos/core#',\n",
    "    'pato': 'http://purl.obolibrary.org/obo/',\n",
    "    'sio': 'http://semanticscience.org/resource/',\n",
    "    'pmid': 'https://www.ncbi.nlm.nih.gov/pubmed/',\n",
    "    'encode': 'https://www.encodeproject.org/search/?searchTerm='\n",
    "}\n",
    "for i, row in statements.iterrows():\n",
    "    if ':' in str(row['property_uri']):\n",
    "        property_uri = row['property_uri']\n",
    "    elif ':' in str(row['property_id']):\n",
    "        try:\n",
    "            property_uri = curie_dct[row['property_id'].split(':')[0].lower()]+row['property_id'].replace(':','_')\n",
    "        except KeyError:\n",
    "            property_uri = None\n",
    "            print('There is a reference curie with and unrecognized namespace:', row['property_id'])\n",
    "    else:\n",
    "        property_uri = None\n",
    "    statements.at[i, 'property_uri'] = property_uri\n",
    "    \n",
    "# save graph\n",
    "print('\\nSaving final graph...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "statements = statements[['subject_id', 'property_id', 'object_id', 'reference_uri',\n",
    "       'reference_supporting_text', 'reference_date', 'property_label',\n",
    "       'property_description', 'property_uri']]\n",
    "print(statements.shape)\n",
    "print(statements.columns)\n",
    "statements.fillna('NA').to_csv('{}/graph_pre_monarch_connectivity_edges_v{}.csv'.format(path,today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "* Graph nodes: add name, synonyms (alias), and description (summary) from biothings, add in case the value is None or NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.insert(0,'/home/nuria/soft/utils3/ontologies')\n",
    "sys.path.insert(0,'./utils')\n",
    "import mondo_class as mondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing curated nodes...\n",
      "\n",
      "ID conversion: from ngly1 curated network to monarch graph...\n",
      "\n",
      "Mapping genes to HGNC ID...\n",
      "querying 1-18...done.\n",
      "Finished.\n",
      "\n",
      "Adding diseases described by the MONDO ontology...\n",
      "\n",
      "Adding BioThings annotation: gene names...\n",
      "symbols: 298\n",
      "querying 1-298...done.\n",
      "Finished.\n",
      "38 input query terms found dup hits:\n",
      "\t[('or', 5), ('NGLY1', 3), ('of', 16), ('1', 14), ('by', 9), ('MRS', 9), ('CSF', 8), ('acid', 4), ('B\n",
      "591 input query terms found no hit:\n",
      "\t['NGLY1-deficiency', 'misfolded', 'incompletely', 'synthesized', 'protein', 'catabolic', 'process', \n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "(298, 6)\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'description',\n",
      "       'name'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing Monarch nodes...\n",
      "\n",
      "Adding BioThings annotation: gene name, synonyms, description...\n",
      "symbols: 1596\n",
      "querying 1-1000...done.\n",
      "querying 1001-1596...done.\n",
      "Finished.\n",
      "289 input query terms found dup hits:\n",
      "\t[('Aqp7', 2), ('Gk2', 3), ('CREB3', 7), ('Creb3', 2), ('capn1', 2), ('Capn1', 2), ('Krt31', 2), ('Ac\n",
      "52 input query terms found no hit:\n",
      "\t['CABZ01053588.1', 'si:dkey-242e21.3', 'T19D2.1', 'ENSEMBL:ENSRNOG00000046782', 'Aqp1<tm1Ask>/Aqp1<t\n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "(4644, 6)\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'description',\n",
      "       'name'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing transcriptomics nodes...\n",
      "(386, 6)\n",
      "Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing tf-gene nodes...\n",
      "(16963, 6)\n",
      "Index(['description', 'id', 'name', 'preflabel', 'semantic_groups',\n",
      "       'synonyms'],\n",
      "      dtype='object')\n",
      "\n",
      "Preparing encoding genes from ngly1 curated network...\n",
      "\n",
      "Adding BioThings annotation: gene symbol, name, synonyms, description...\n",
      "querying 1-43...done.\n",
      "Finished.\n",
      "\n",
      "Annotating nodes in the graph...\n",
      "(9365, 1)\n",
      "\n",
      "Concatenating all nodes...\n",
      "(9594, 6)\n",
      "\n",
      "Drop duplicated rows...\n",
      "(9475, 6)\n",
      "\n",
      "Drop duplicated nodes...\n",
      "(9364, 6)\n",
      "\n",
      "There is a problem in the annotation of nodes.\n",
      "The number of annotated nodes is different than the number of nodes in the graph.\n",
      "Curated nodes not in the graph: set()\n",
      "Monarch nodes not in the graph: set()\n",
      "RNA-seq nodes not in the graph: set()\n",
      "Regulation nodes not in the graph: 12738\n",
      "\n",
      "Saving final graph...\n",
      "(9364, 6)\n",
      "Index(['id', 'semantic_groups', 'preflabel', 'synonyms', 'name',\n",
      "       'description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# working directory\n",
    "path = os.getcwd() + \"/graph/\" + version\n",
    "\n",
    "## curated nodes\n",
    "print('\\nPreparing curated nodes...')\n",
    "# concat all curated concepts in the network\n",
    "df_l = []\n",
    "for file in glob.glob('{}/*_nodes.tsv'.format(path)):\n",
    "    with open(file, 'r') as f:\n",
    "        df_l.append(pd.read_table(f))\n",
    "\n",
    "curated_df = pd.concat(df_l, ignore_index=True, join=\"inner\")   \n",
    "\n",
    "# ID curie normalization: curation to Monarch ID\n",
    "curated_df['id'] = ( curated_df.id\n",
    "               .apply(lambda x:\n",
    "                      'ClinVarVariant:50962' if 'HGVS' in str(x) else     \n",
    "                       x.replace('Reactome', 'REACT') if 'Reactome' in str(x) else str(x).strip()\n",
    "                     )\n",
    ")\n",
    "\n",
    "# uniform columns bw monarch and curated file formats\n",
    "curated_df = curated_df[['id', 'semantic_groups', 'preflabel', 'synonyms', 'description']]\n",
    "\n",
    "# drop duplicates\n",
    "curated_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# save curated nodes\n",
    "curated_df.fillna('NA').to_csv('{}/curated_nodes_v{}.csv'.format(path,today), index=False)\n",
    "\n",
    "# ID conversion: from ngly1 curated network to monarch graph \n",
    "# script: http://localhost:8888/notebooks/workspace/ngly1-graph/regulation/curated.ipynb\n",
    "# HUMAN GENES: NCBIGene to HGNC ID using biothings\n",
    "# DISEASES: DO, OMIM, Orphanet IDs to MONDO ID manually \n",
    "# and add edges disease id to mondo id in the graph (exact match)\n",
    "# and add new mondo id nodes parsing the mondo owl ontology to extract node attributes\n",
    "print('\\nID conversion: from ngly1 curated network to monarch graph...')\n",
    "# Genes #\n",
    "print('\\nMapping genes to HGNC ID...')\n",
    "# biothings api + dictionaries \n",
    "# api input\n",
    "entrez = list()\n",
    "for i, row in curated_df.iterrows():\n",
    "    if ':' in row['id']:\n",
    "        if 'ncbigene' in row['id'].split(':')[0].lower():\n",
    "            entrez.append(row['id'].split(':')[1])\n",
    "entrez = list(set(entrez))\n",
    "\n",
    "# api call \n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(entrez, scopes = 'entrezgene', fields='HGNC', size=1, as_dataframe=True)\n",
    "\n",
    "# build dictionary\n",
    "ids = df.reset_index().rename(columns={'query': 'entrez'}).copy()\n",
    "entrez2hgnc_dct = dict(zip(ids.entrez, ids.HGNC))\n",
    "\n",
    "# map to hgnc\n",
    "lines = []\n",
    "for idx, row in curated_df.iterrows():\n",
    "    # subject\n",
    "    if ':' in row['id']:\n",
    "        if 'ncbigene' in row['id'].split(':')[0].lower():\n",
    "            # human ncbi gene ids with HGNC ID\n",
    "            if str(entrez2hgnc_dct[row['id'].split(':')[1]]) != 'nan':\n",
    "                row['id'] = \"HGNC:\"+entrez2hgnc_dct[row['id'].split(':')[1]]\n",
    "            # specific non human ncbi gene ids in the curated set\n",
    "            elif row['id'] == 'NCBIGene:173028':\n",
    "                row['id'] = 'WormBase:WBGene00010160'\n",
    "            elif row['id'] == 'NCBIGene:11826':\n",
    "                row['id'] = 'MGI:103201'\n",
    "\n",
    "    lines.append((row))\n",
    "curated_df = pd.DataFrame.from_records(lines)\n",
    "\n",
    "# Diseases #\n",
    "print('\\nAdding diseases described by the MONDO ontology...')\n",
    "# import mondo owl terms\n",
    "owl_f = os.getcwd() + '/../ontologies/mondo.owl'\n",
    "tm = mondo.term(owl_f)\n",
    "# extract metadata from the mondo ontology\n",
    "nodes_l = list()\n",
    "for disease, mondo in d2m.items():\n",
    "    mondo_term = tm.get_metadata_per_id(id=mondo)\n",
    "    node = dict()\n",
    "    node['id'] = mondo_term['id']\n",
    "    node['semantic_groups'] = 'DISO'\n",
    "    node['preflabel'] = mondo_term['label']\n",
    "    node['synonyms'] = mondo_term['synonyms']\n",
    "    node['description'] = mondo_term['definition']\n",
    "    nodes_l.append(node)\n",
    "    \n",
    "# add disease nodes to curated_df\n",
    "d2m_nodes_df = pd.DataFrame(nodes_l)\n",
    "d2m_nodes_df.drop_duplicates(inplace=True)\n",
    "curated_df = pd.concat([curated_df,d2m_nodes_df], ignore_index=True, join=\"inner\")\n",
    "\n",
    "# biothings: annotate name to genes\n",
    "print('\\nAdding BioThings annotation: gene names...')\n",
    "# input: (preflabel) symbol,alias\n",
    "symbols = list(curated_df.preflabel)\n",
    "print('symbols:', len(symbols))\n",
    "\n",
    "# query biothings\n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(symbols, scopes = 'symbol,alias', fields='name', size=1, as_dataframe=True)\n",
    "\n",
    "# dictionary: {symbol:name}\n",
    "ids = ( df.reset_index().rename(columns={'query':'symbol'}) )\n",
    "#ids['names'] = ids.name.apply(lambda x: x if str(x) != 'nan' else 'NA')\n",
    "#curated_s2n = dict(zip(ids.symbol, ids.names))\n",
    "#curated_df['name'] = curated_df.preflabel.apply(lambda x: curated_s2n[x] if x in curated_s2n.keys() else 'NA')\n",
    "curated_s2n = dict(zip(ids.symbol, ids.name))\n",
    "curated_df['name'] = curated_df.preflabel.apply(lambda x: curated_s2n[x] if x in curated_s2n.keys() else x)\n",
    "\n",
    "# save curated nodes\n",
    "curated_df.fillna('NA').to_csv('{}/curated_graph_nodes_v{}.csv'.format(path,today), index=False)\n",
    "print(curated_df.shape)\n",
    "print(curated_df.columns)\n",
    "\n",
    "## monarch nodes\n",
    "print('\\nPreparing Monarch nodes...')\n",
    "path = os.getcwd() + \"/../monarch/1shell-animal/add-connections-to-net\"\n",
    "monarch_df = pd.read_table('{}/monarch_nodes_v2019-01-16.tsv'.format(path))\n",
    "\n",
    "# biothings: annotate name,synonyms,description to genes\n",
    "print('\\nAdding BioThings annotation: gene name, synonyms, description...')\n",
    "# input: (preflabel) symbol,alias\n",
    "symbols = list()\n",
    "for i, row in monarch_df.iterrows():\n",
    "    if isinstance(row['semantic_groups'],list):\n",
    "        for label in row['semantic_groups']:\n",
    "            if 'GENE' in label:\n",
    "                symbols.append(row['preflabel'])\n",
    "    else:\n",
    "        if 'GENE' in row['semantic_groups']:\n",
    "            symbols.append(row['preflabel'])\n",
    "print('symbols:', len(symbols))\n",
    "\n",
    "# query biothings\n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(symbols, scopes = 'symbol,alias', fields='name,alias,summary', size=1, as_dataframe=True)\n",
    "\n",
    "# dictionary: {symbol:name}\n",
    "ids = ( df.reset_index().rename(columns={'query':'symbol'}) )\n",
    "#ids['names'] = ids.name.apply(lambda x: x if str(x) != 'nan' else 'NA')\n",
    "ids['synonyms'] = ids.alias.apply(lambda x: x if str(x) != 'nan' else 'NA')\n",
    "ids['description'] = ids.summary.apply(lambda x: x if str(x) != 'nan' else 'NA')\n",
    "#monarch_s2n = dict(zip(ids.symbol, ids.names))\n",
    "monarch_s2n = dict(zip(ids.symbol, ids.name))\n",
    "monarch_s2s = dict(zip(ids.symbol, ids.synonyms))\n",
    "monarch_s2d = dict(zip(ids.symbol, ids.description))\n",
    "#monarch_df['name'] = monarch_df.preflabel.apply(lambda x: monarch_s2n[x] if x in monarch_s2n.keys() else 'NA')\n",
    "monarch_df['name'] = monarch_df.preflabel.apply(lambda x: monarch_s2n[x] if x in monarch_s2n.keys() else x)\n",
    "monarch_df['synonyms'] = monarch_df.preflabel.apply(lambda x: monarch_s2s[x] if x in monarch_s2s.keys() else 'NA')\n",
    "monarch_df['description'] = monarch_df.preflabel.apply(lambda x: monarch_s2d[x] if x in monarch_s2d.keys() else 'NA')\n",
    "print(monarch_df.shape)\n",
    "print(monarch_df.columns)\n",
    "\n",
    "## rna nodes\n",
    "print('\\nPreparing transcriptomics nodes...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "rna_df = pd.read_csv('{}/rna_nodes_v2019-01-17.csv'.format(path))\n",
    "print(rna_df.shape)\n",
    "print(rna_df.columns)\n",
    "\n",
    "## tf nodes: i only want tf merged nodes annotated\n",
    "print('\\nPreparing tf-gene nodes...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "tf_df = pd.read_csv('{}/regulation_nodes_v2019-01-17.csv'.format(path))\n",
    "print(tf_df.shape)\n",
    "print(tf_df.columns)\n",
    "\n",
    "## Annotating curated gene nodes from P-G edges\n",
    "print('\\nPreparing encoding genes from ngly1 curated network...')\n",
    "# biothings api + dictionaries \n",
    "print('\\nAdding BioThings annotation: gene symbol, name, synonyms, description...')\n",
    "# api input\n",
    "uniprot = set()\n",
    "for i, row in curated_df.iterrows():\n",
    "    if ':' in row['id']:\n",
    "        if 'uniprot' in row['id'].split(':')[0].lower():\n",
    "            uniprot.add(row['id'].split(':')[1])\n",
    "\n",
    "# api call \n",
    "mg = get_client('gene')\n",
    "df = mg.querymany(uniprot, scopes = 'uniprot', fields='HGNC,symbol,name,alias,summary', size=1, as_dataframe=True)\n",
    "\n",
    "# build a list of nodes as list of dict, i.e a df, where a dict is a node\n",
    "nodes_l = list()\n",
    "for i,concept in df.iterrows():\n",
    "    if str(concept['HGNC']) != 'nan':\n",
    "        node = dict()\n",
    "        node['id'] = 'HGNC:'+concept['HGNC']\n",
    "        node['semantic_groups'] = 'GENE'\n",
    "        node['preflabel'] = concept['symbol'] \n",
    "        node['name'] = concept['name'] \n",
    "        node['synonyms'] = '|'.join(list(concept['alias'])) if isinstance(concept['alias'], list) else concept['alias']\n",
    "        node['description'] = concept['summary']\n",
    "        nodes_l.append(node)\n",
    "    \n",
    "# structure as dataframe\n",
    "p2g_nodes_df = pd.DataFrame(nodes_l)\n",
    "p2g_nodes_df = p2g_nodes_df.fillna('NA')\n",
    "p2g_nodes_df.drop_duplicates(inplace=True)\n",
    "\n",
    "## Annotating nodes in the graph\n",
    "print('\\nAnnotating nodes in the graph...')\n",
    "# extracting nodes in the graph\n",
    "st_nodes_l = pd.concat([statements.subject_id,statements.object_id], ignore_index=True)\n",
    "st_nodes_l.drop_duplicates(inplace=True)\n",
    "st_nodes_df = pd.DataFrame({'id': st_nodes_l})\n",
    "print(st_nodes_df.shape)\n",
    "\n",
    "# annotating nodes \n",
    "curated_nodes = pd.merge(curated_df,st_nodes_df,how='inner',on='id')\n",
    "monarch_nodes = pd.merge(monarch_df,st_nodes_df,how='inner',on='id')\n",
    "rna_nodes = pd.merge(rna_df,st_nodes_df,how='inner',on='id')\n",
    "regulation_nodes = pd.merge(tf_df,st_nodes_df,how='inner', on='id')\n",
    "p2g_nodes = pd.merge(p2g_nodes_df,st_nodes_df,how='inner', on='id')\n",
    "\n",
    "# concat all, (importantly, concatenate first curated concepts with extended definitions)\n",
    "print('\\nConcatenating all nodes...')\n",
    "nodes = pd.concat([curated_nodes,monarch_nodes,rna_nodes,regulation_nodes,p2g_nodes], ignore_index=True, join=\"inner\")\n",
    "print(nodes.shape)\n",
    "\n",
    "# drop duplicated rows\n",
    "print('\\nDrop duplicated rows...')\n",
    "nodes['synonyms'] = nodes.synonyms.apply(lambda x: str('|'.join(x)) if isinstance(x,list) else x)\n",
    "nodes.drop_duplicates(keep='first',inplace=True)\n",
    "print(nodes.shape)\n",
    "\n",
    "# drop duplicated nodes (keep first row (the curated), remove others (monarch))\n",
    "print('\\nDrop duplicated nodes...')\n",
    "nodes.drop_duplicates(subset=['id'],keep='first',inplace=True)\n",
    "print(nodes.shape)\n",
    "\n",
    "# check\n",
    "if len(set(st_nodes_df.id)) != len(set(nodes.id)):\n",
    "    print('\\nThere is a problem in the annotation of nodes.\\nThe number of annotated nodes is different than the number of nodes in the graph.')\n",
    "    print('Curated nodes not in the graph: {}'.format(set(curated_df.id)-set(curated_nodes.id)))\n",
    "    print('Monarch nodes not in the graph: {}'.format(set(monarch_df.id)-set(monarch_nodes.id)))\n",
    "    print('RNA-seq nodes not in the graph: {}'.format(set(rna_df.id)-set(rna_nodes.id)))\n",
    "    print('Regulation nodes not in the graph: {}'.format(len(set(tf_df.id)-set(regulation_nodes.id))))\n",
    "else:\n",
    "    print('\\nAll graph nodes are annotated.')\n",
    "    print('Regulation nodes not in the graph: {}'.format(len(set(tf_df.id)-set(regulation_nodes.id))))\n",
    "          \n",
    "## biothings\n",
    "# add attributes\n",
    "\n",
    "# all genes/proteins => add entrez|uniprot\n",
    "\n",
    "# save graph nodes\n",
    "print('\\nSaving final graph...')\n",
    "path = os.getcwd() + \"/graph\"\n",
    "nodes = nodes[['id','semantic_groups','preflabel','synonyms','name','description']]\n",
    "nodes['synonyms'] = nodes.synonyms.apply(lambda x: str('|'.join(x)) if isinstance(x,list) else x)\n",
    "print(nodes.shape)\n",
    "print(nodes.columns)\n",
    "nodes.fillna('NA').to_csv('{}/graph_pre_monarch_connectivity_nodes_v{}.csv'.format(path,today), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
